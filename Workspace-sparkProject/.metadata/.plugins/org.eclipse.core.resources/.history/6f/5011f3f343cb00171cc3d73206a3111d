package com.yatish.spark.transformations;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;

import com.yatish.spark.partitioner.Custom_Partitioner;

import scala.Tuple2;

public class S11_GroupByKey {

	public static void main(String[] args) {
		
		SparkConf conf = new SparkConf().setAppName("SparkStart").setMaster("local");
		JavaSparkContext sc = new JavaSparkContext(conf);
		List<Integer> data = Arrays.asList(1, 2, 3, 4, 5, 6, 1, 2, 3);	
		JavaRDD<Integer> rddData = sc.parallelize(data);
		
		JavaPairRDD<Integer,Integer> rddData1 = rddData.mapToPair(a -> {
			return new Tuple2<Integer,Integer>(a,1);
		});
		
		/*
		 * Consider, you have a set of key value pairs, there are duplicate keys in that set. spark provides a method '.groupByKey()' to covert duplicate keys to one entry.
		 * 
		 * In this case, 
		 * 	'rddData1'  --> [(1,1), (2,1), (3,1), (4,1), (5,1), (6,1), (1,1), (2,4), (3,1)]
		 * now if we call '.groupByKey()' method on 'rddData1'. then the result assigned to 'rddData2' will be as follows,
		 * 	'rddData2'  --> [(4,[1]), (1,[1, 1]), (6,[1]), (3,[1, 1]), (5,[1]), (2,[1, 4])]
		 * in 'rddData1' we had two entries with same key '2' which are (2,1) and (2,4). that is converted to (2,[1, 4]) in 'rddData2'. Even if there are no duplicate keys also they will be converted
		 * this way like there is only one entry with key as '4' i.e. (4,1)  this will also be converted as (4,[1])
		 * 	
		 */
		JavaPairRDD<Integer,Iterable<Integer>> rddData2 = rddData1.groupByKey();
		
		
		/*
		 * There is one more variation of '.groupByKey()' method which is '.groupByKey(int numOfPartitions)', based on the argument passed those many partitions will be created and returned.
		 * 
		 * in this case, there will be 5 partitions created and returned to 'rddData3'
		 */
		JavaPairRDD<Integer,Iterable<Integer>> rddData3 = rddData1.groupByKey(5);
		
		
		/*
		 * There is one more variation of '.groupByKey()' method which is '.groupByKey(Partitioner obj)', we can pass the partitioner which we want to be used for partitioning, it may be custom or 
		 * spark provided partitioners.
		 * 
		 */
		rddData1.groupByKey(new Custom_Partitioner(4));
		
		
		
	
		List<String> data1 = Arrays.asList("yatish, 1,2","ramya, 5 ,6 ","yatish,7,8");	
		JavaRDD<String> rdd = sc.parallelize(data1);
		/*
		 * using '.groupByKey()' we can group the values based on only 'key'. consider you want group the keys based on values how will you do that?
		 * Spark provides one more method '.groupBy'. using this we can group elements of RDD based on anything.
		 * 
		 * In the below code, we are grouping elements of RDD based on the logic defined within '.groupBy' method as argument. In the below code, we are grouping elements of RDD based on the 
		 * first string before ',' of RDD elements. 
		 * 'group' --> [(ramya,[ramya, 5 ,6 ]), (yatish,[yatish, 1,2, yatish,7,8])]
		 * Since, we are returning the first string before ','. that will be considered as the key to group the RDD elements. check the above value assigned to 'group', you will understand clearly.
		 * 
		 * NOTE: whatever is returned from the lambda/Function will be considered as the key for grouping.
		 * NOTE: The return type will be 'JavaPairRDD<String, Iterable<String>>', the first type will be the key type that is returned from the lambda/Function. The second type will be the java
		 * 		source RDD type. i.e. in this case 'rdd' type.
		 */
		JavaPairRDD<String, Iterable<String>> group = rdd.groupBy(a -> {
			String[] b = a.split(",");
			return b[0];
			});
	
		
		
		
		/*
		 * The same thing which is implemented in above code is implemented in java function instead of lambda.
		 */
		JavaPairRDD<String, Iterable<String>> group1 = rdd.groupBy(new Function<String, String>() {
			@Override
			public String call(String arg0) throws Exception {
			String[] data = arg0.split(",");
			return data[0];
			}
			});

		
		
		
		/*
		 * One more example of '.groupBy' method. In this we are trying to group the elements of the RDD considering the values as key'. Hence we are returning the second value of tuple  which is
		 * nothing but the value in the key value pair.
		 * 
		 * 'rddData4' --> [(1,[(1,1), (2,1), (3,1), (4,1), (5,1), (6,1), (1,1), (2,1), (3,1)])]
		 */
		JavaPairRDD<Integer, Iterable<Tuple2<Integer,Integer>>> rddData4 = rddData1.groupBy(new Function<Tuple2<Integer,Integer>,Integer>() {
			
			public Integer call(Tuple2<Integer,Integer> in) {
				return in._2();
			}
		});
		
		
		JavaPairRDD<Integer,Tuple<Iterator<Integer>,Itarator<Integer>>> rddData5 = rddData1.groupWith(rddData4);
	
	}

}
