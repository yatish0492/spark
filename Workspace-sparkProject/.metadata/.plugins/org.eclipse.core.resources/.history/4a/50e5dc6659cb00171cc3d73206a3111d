package com.yatish.spark.transformations;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function2;

import scala.Tuple2;

public class S12_ReduceByKey {

	public static void main(String[] args) {
		
		SparkConf conf = new SparkConf().setAppName("SparkStart").setMaster("local");
		JavaSparkContext sc = new JavaSparkContext(conf);
		List<Integer> data = Arrays.asList(1, 2, 3, 4, 5, 6, 1, 2, 3,1);	
		JavaRDD<Integer> rddData = sc.parallelize(data);
		
		JavaPairRDD<Integer,Integer> rddData1 = rddData.mapToPair(a -> {
			return new Tuple2<Integer,Integer>(a,2);
		});
		
		/*
		 * Using '.groupByKey()' method you are just grouping the results instead of two pairs like (2,3) and (2,5) it will give you (2,[3,5]). Consider instead of assigning both the values to same
		 * key in one pair you want to do some operations on the values and assign it like instead of (2,[3,5]), you want it to be (2,8), we add the values 3 and 5 and assign it as the value instead
		 * of maintain a list as value?
		 * Using '.reduceByKey' we can achieve it. We basically do reduce/action on the values.
		 * 
		 * 
		 * The logic will be executed for only pairs with duplicate keys.
		 * [(4,1), (1,1), (6,1), (3,1), (5,1), (2,1)]
		 * [(4,1), (1,2), (6,1), (3,2), (5,1), (2,2)]
		 */
		
		JavaPairRDD<Integer,Integer> rddData2 = rddData1.reduceByKey((valueOfPair1,valueOfPair2) -> valueOfPair1+valueOfPair2;);
		
		JavaPairRDD<Integer,Integer> rddData3 = rddData1.reduceByKey(new Function2<Integer,Integer,Integer>() {
			
			public Integer call(Integer a,Integer b) {
				System.out.println("a " + a);
				System.out.println("b " + b);
				return a+b;
			}
		});
		System.out.println(rddData3.collect());

	}

}
